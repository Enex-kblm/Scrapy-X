{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraped Data Analysis\n",
    "\n",
    "This notebook demonstrates how to analyze data scraped using the web scraper framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Scraped Data\n",
    "\n",
    "Load the latest scraped data from the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directories\n",
    "data_dir = Path(\"../data\")\n",
    "raw_data_dir = data_dir / \"raw\"\n",
    "processed_data_dir = data_dir / \"processed\"\n",
    "\n",
    "# Find the latest JSON file\n",
    "json_files = list(raw_data_dir.glob(\"*.json\"))\n",
    "if json_files:\n",
    "    latest_file = max(json_files, key=lambda x: x.stat().st_mtime)\n",
    "    print(f\"Loading data from: {latest_file}\")\n",
    "    \n",
    "    with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Loaded {len(df)} records\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "else:\n",
    "    print(\"No JSON files found in the raw data directory.\")\n",
    "    print(\"Please run the scraper first to generate data.\")\n",
    "    # Create sample data for demonstration\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"id\": f\"sample_{i}\",\n",
    "            \"text\": f\"This is sample tweet {i} with #hashtag\",\n",
    "            \"created_at\": f\"2024-01-{15+i%10:02d}T{10+i%12:02d}:30:00Z\",\n",
    "            \"like_count\": np.random.randint(0, 100),\n",
    "            \"retweet_count\": np.random.randint(0, 50),\n",
    "            \"reply_count\": np.random.randint(0, 20),\n",
    "            \"lang\": np.random.choice([\"en\", \"es\", \"fr\"], p=[0.7, 0.2, 0.1]),\n",
    "            \"hashtags\": [\"#hashtag\", \"#sample\"] if i % 3 == 0 else [\"#hashtag\"],\n",
    "            \"text_length\": len(f\"This is sample tweet {i} with #hashtag\")\n",
    "        }\n",
    "        for i in range(100)\n",
    "    ]\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(\"Created sample data for demonstration\")\n",
    "\n",
    "# Display basic information\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Convert date columns if they exist\n",
    "if 'created_at' in df.columns:\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    df['date'] = df['created_at'].dt.date\n",
    "    df['hour'] = df['created_at'].dt.hour\n",
    "    df['day_of_week'] = df['created_at'].dt.day_name()\n",
    "\n",
    "print(\"\\nDataset after date processing:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engagement Metrics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate engagement metrics\n",
    "if all(col in df.columns for col in ['like_count', 'retweet_count', 'reply_count']):\n",
    "    df['total_engagement'] = df['like_count'] + df['retweet_count'] + df['reply_count']\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"Engagement Statistics:\")\n",
    "    engagement_cols = ['like_count', 'retweet_count', 'reply_count', 'total_engagement']\n",
    "    print(df[engagement_cols].describe())\n",
    "    \n",
    "    # Visualize engagement distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Engagement Metrics Distribution', fontsize=16)\n",
    "    \n",
    "    for i, col in enumerate(engagement_cols):\n",
    "        row, col_idx = i // 2, i % 2\n",
    "        ax = axes[row, col_idx]\n",
    "        \n",
    "        # Histogram\n",
    "        df[col].hist(bins=20, ax=ax, alpha=0.7)\n",
    "        ax.set_title(f'{col.replace(\"_\", \" \").title()} Distribution')\n",
    "        ax.set_xlabel(col.replace('_', ' ').title())\n",
    "        ax.set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'created_at' in df.columns:\n",
    "    # Posts by day\n",
    "    daily_posts = df.groupby('date').size().reset_index(name='post_count')\n",
    "    \n",
    "    # Posts by hour\n",
    "    hourly_posts = df.groupby('hour').size().reset_index(name='post_count')\n",
    "    \n",
    "    # Posts by day of week\n",
    "    weekly_posts = df.groupby('day_of_week').size().reset_index(name='post_count')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    # Daily timeline\n",
    "    axes[0].plot(daily_posts['date'], daily_posts['post_count'], marker='o')\n",
    "    axes[0].set_title('Posts Over Time')\n",
    "    axes[0].set_xlabel('Date')\n",
    "    axes[0].set_ylabel('Number of Posts')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hourly distribution\n",
    "    axes[1].bar(hourly_posts['hour'], hourly_posts['post_count'])\n",
    "    axes[1].set_title('Posts by Hour of Day')\n",
    "    axes[1].set_xlabel('Hour')\n",
    "    axes[1].set_ylabel('Number of Posts')\n",
    "    \n",
    "    # Day of week distribution\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    weekly_posts['day_of_week'] = pd.Categorical(weekly_posts['day_of_week'], categories=day_order, ordered=True)\n",
    "    weekly_posts = weekly_posts.sort_values('day_of_week')\n",
    "    \n",
    "    axes[2].bar(weekly_posts['day_of_week'], weekly_posts['post_count'])\n",
    "    axes[2].set_title('Posts by Day of Week')\n",
    "    axes[2].set_xlabel('Day of Week')\n",
    "    axes[2].set_ylabel('Number of Posts')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language and Content Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language distribution\n",
    "if 'lang' in df.columns:\n",
    "    lang_counts = df['lang'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Language pie chart\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.pie(lang_counts.values, labels=lang_counts.index, autopct='%1.1f%%')\n",
    "    plt.title('Language Distribution')\n",
    "    \n",
    "    # Language bar chart\n",
    "    plt.subplot(1, 2, 2)\n",
    "    lang_counts.plot(kind='bar')\n",
    "    plt.title('Posts by Language')\n",
    "    plt.xlabel('Language')\n",
    "    plt.ylabel('Number of Posts')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Text length analysis\n",
    "if 'text_length' in df.columns:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    df['text_length'].hist(bins=30, alpha=0.7)\n",
    "    plt.title('Text Length Distribution')\n",
    "    plt.xlabel('Text Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    df.boxplot(column='text_length', ax=plt.gca())\n",
    "    plt.title('Text Length Box Plot')\n",
    "    plt.ylabel('Text Length (characters)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Average text length: {df['text_length'].mean():.1f} characters\")\n",
    "    print(f\"Median text length: {df['text_length'].median():.1f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'hashtags' in df.columns:\n",
    "    # Extract all hashtags\n",
    "    all_hashtags = []\n",
    "    for hashtag_list in df['hashtags']:\n",
    "        if isinstance(hashtag_list, list):\n",
    "            all_hashtags.extend(hashtag_list)\n",
    "    \n",
    "    # Count hashtag frequency\n",
    "    hashtag_counts = pd.Series(all_hashtags).value_counts()\n",
    "    \n",
    "    print(f\"Total unique hashtags: {len(hashtag_counts)}\")\n",
    "    print(f\"Total hashtag mentions: {len(all_hashtags)}\")\n",
    "    \n",
    "    # Plot top hashtags\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    top_hashtags = hashtag_counts.head(15)\n",
    "    plt.barh(range(len(top_hashtags)), top_hashtags.values)\n",
    "    plt.yticks(range(len(top_hashtags)), top_hashtags.index)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.title('Top 15 Hashtags')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Hashtag statistics\n",
    "    df['hashtag_count'] = df['hashtags'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    df['hashtag_count'].value_counts().sort_index().plot(kind='bar')\n",
    "    plt.title('Distribution of Number of Hashtags per Post')\n",
    "    plt.xlabel('Number of Hashtags')\n",
    "    plt.ylabel('Number of Posts')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Average hashtags per post: {df['hashtag_count'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns for correlation analysis\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numeric_cols) > 1:\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                fmt='.2f')\n",
    "    plt.title('Correlation Matrix of Numeric Variables')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print strong correlations\n",
    "    print(\"Strong correlations (|r| > 0.5):\")\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.5:\n",
    "                col1 = correlation_matrix.columns[i]\n",
    "                col2 = correlation_matrix.columns[j]\n",
    "                print(f\"{col1} <-> {col2}: {corr_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Analytics and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engagement rate by text length\n",
    "if all(col in df.columns for col in ['text_length', 'total_engagement']):\n",
    "    # Create text length bins\n",
    "    df['text_length_bin'] = pd.cut(df['text_length'], \n",
    "                                  bins=5, \n",
    "                                  labels=['Very Short', 'Short', 'Medium', 'Long', 'Very Long'])\n",
    "    \n",
    "    engagement_by_length = df.groupby('text_length_bin')['total_engagement'].mean()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    engagement_by_length.plot(kind='bar')\n",
    "    plt.title('Average Engagement by Text Length')\n",
    "    plt.xlabel('Text Length Category')\n",
    "    plt.ylabel('Average Total Engagement')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Engagement by hashtag count\n",
    "if all(col in df.columns for col in ['hashtag_count', 'total_engagement']):\n",
    "    engagement_by_hashtags = df.groupby('hashtag_count')['total_engagement'].mean()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    engagement_by_hashtags.plot(kind='bar')\n",
    "    plt.title('Average Engagement by Number of Hashtags')\n",
    "    plt.xlabel('Number of Hashtags')\n",
    "    plt.ylabel('Average Total Engagement')\n",
    "    plt.show()\n",
    "\n",
    "# Language vs engagement\n",
    "if all(col in df.columns for col in ['lang', 'total_engagement']):\n",
    "    engagement_by_lang = df.groupby('lang')['total_engagement'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    engagement_by_lang.plot(kind='bar')\n",
    "    plt.title('Average Engagement by Language')\n",
    "    plt.xlabel('Language')\n",
    "    plt.ylabel('Average Total Engagement')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary statistics\n",
    "analysis_summary = {\n",
    "    'total_records': len(df),\n",
    "    'date_range': {\n",
    "        'start': df['created_at'].min().strftime('%Y-%m-%d') if 'created_at' in df.columns else None,\n",
    "        'end': df['created_at'].max().strftime('%Y-%m-%d') if 'created_at' in df.columns else None\n",
    "    },\n",
    "    'languages': df['lang'].value_counts().to_dict() if 'lang' in df.columns else {},\n",
    "    'average_engagement': df['total_engagement'].mean() if 'total_engagement' in df.columns else None,\n",
    "    'top_hashtags': hashtag_counts.head(10).to_dict() if 'hashtags' in df.columns else {},\n",
    "    'text_stats': {\n",
    "        'avg_length': df['text_length'].mean() if 'text_length' in df.columns else None,\n",
    "        'median_length': df['text_length'].median() if 'text_length' in df.columns else None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save analysis summary\n",
    "summary_file = processed_data_dir / f\"analysis_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(analysis_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Analysis summary saved to: {summary_file}\")\n",
    "print(\"\\nAnalysis Summary:\")\n",
    "for key, value in analysis_summary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights and Recommendations\n",
    "\n",
    "Based on the analysis above, here are some key insights:\n",
    "\n",
    "1. **Temporal Patterns**: Look for peak posting times and days to optimize content timing\n",
    "2. **Engagement Drivers**: Identify what content characteristics lead to higher engagement\n",
    "3. **Language Distribution**: Understand your audience's language preferences\n",
    "4. **Hashtag Strategy**: Use top-performing hashtags and optimal hashtag counts\n",
    "5. **Content Length**: Find the sweet spot for text length that maximizes engagement\n",
    "\n",
    "This analysis framework can be extended with:\n",
    "- Sentiment analysis\n",
    "- Topic modeling\n",
    "- User network analysis\n",
    "- Predictive modeling for engagement\n",
    "- Anomaly detection for viral content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}